{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","authorship_tag":"ABX9TyMk8XHmZYugIKWIXBCWRSAC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5__njjTMdS6z","executionInfo":{"status":"ok","timestamp":1731182244114,"user_tz":-480,"elapsed":2309,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"e70c9cb9-d116-4dcf-cab1-8419ef1bd451"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from tqdm import tqdm\n","import numpy as np"],"metadata":{"id":"54IggHXrdo3M","executionInfo":{"status":"ok","timestamp":1731182248166,"user_tz":-480,"elapsed":2466,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","from torchvision import datasets, transforms\n","import torch\n","\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/EE6483Project/datasets/datasets/train\"\n","val_dir = \"/content/drive/MyDrive/Colab Notebooks/EE6483Project/datasets/datasets/val\"\n","\n","data_transform = {\n","    \"train\": transforms.Compose([\n","        transforms.Resize([224, 224]),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ]),\n","    \"val\": transforms.Compose([\n","        transforms.Resize([224, 224]),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","}\n","\n","\n","image_datasets = {\n","    \"train\": datasets.ImageFolder(root=train_dir, transform=data_transform[\"train\"]),\n","    \"val\": datasets.ImageFolder(root=val_dir, transform=data_transform[\"val\"])\n","}\n","\n","dataloader = {\n","    \"train\": torch.utils.data.DataLoader(dataset=image_datasets[\"train\"], batch_size=16, shuffle=True),\n","    \"val\": torch.utils.data.DataLoader(dataset=image_datasets[\"val\"], batch_size=16, shuffle=False)\n","}\n"],"metadata":{"id":"-tbEPrnKdyRv","executionInfo":{"status":"ok","timestamp":1731182249657,"user_tz":-480,"elapsed":1493,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class DatasetLoader:\n","    def __init__(self, train_dir, val_dir, batch_size=16):\n","        data_transform = {\n","            \"train\": transforms.Compose([\n","                transforms.Resize([224, 224]),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","            ]),\n","            \"val\": transforms.Compose([\n","                transforms.Resize([224, 224]),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","            ])\n","        }\n","\n","        self.image_datasets = {\n","            \"train\": datasets.ImageFolder(root=train_dir, transform=data_transform[\"train\"]),\n","            \"val\": datasets.ImageFolder(root=val_dir, transform=data_transform[\"val\"])\n","        }\n","\n","        self.dataloaders = {\n","            \"train\": DataLoader(self.image_datasets[\"train\"], batch_size=batch_size, shuffle=True),\n","            \"val\": DataLoader(self.image_datasets[\"val\"], batch_size=batch_size, shuffle=False)\n","        }"],"metadata":{"id":"H03AiQUQeAiy","executionInfo":{"status":"ok","timestamp":1731182252578,"user_tz":-480,"elapsed":288,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","import torch.optim as optim\n","\n","\n","class GoogLeNet(nn.Module):\n","    def __init__(self, num_classes=1000, aux_logits=True, init_weights=False):\n","        super(GoogLeNet, self).__init__()\n","        self.aux_logits = aux_logits\n","\n","        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n","        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n","        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n","        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n","        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n","        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n","        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n","        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n","        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n","\n","        if self.aux_logits:\n","            self.aux1 = InceptionAux(512, num_classes)\n","            self.aux2 = InceptionAux(528, num_classes)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.fc = nn.Linear(1024, num_classes)\n","        if init_weights:\n","            self._init_weight()\n","\n","    def forward(self, x):\n","        # N x 3 x 224 x 224\n","        x = self.conv1(x)\n","        # N x 64 x 112 x 112\n","        x = self.maxpool1(x)\n","        # N x 64 x 56 x 56\n","        x = self.conv2(x)\n","        # N x 64 x 56 x 56\n","        x = self.conv3(x)\n","        # N x 192 x 56 x 56\n","        x = self.maxpool2(x)\n","\n","        # N x 192 x 28 x 28\n","        x = self.inception3a(x)\n","        # N x 256 x 28 x 28\n","        x = self.inception3b(x)\n","        # N x 480 x 28 x 28\n","        x = self.maxpool3(x)\n","        # N x 480 x 14 x 14\n","        x = self.inception4a(x)\n","        # N x 512 x 14 x 14\n","        if self.aux_logits and self.training:\n","            aux1 = self.aux1(x)\n","\n","        x = self.inception4b(x)\n","        # N x 512 x 14 x 14\n","        x = self.inception4c(x)\n","        # N x 512 x 14 x 14\n","        x = self.inception4d(x)\n","        # N x 528 x 14 x 14\n","        if self.aux_logits and self.training:\n","            aux2 = self.aux2(x)\n","\n","        x = self.inception4e(x)\n","        # N x 832 x 14 x 14\n","        x = self.maxpool4(x)\n","        # N x 832 x 7 x 7\n","        x = self.inception5a(x)\n","        # N x 832 x 7 x 7\n","        x = self.inception5b(x)\n","        # N x 1024 x 7 x 7\n","\n","        x = self.avgpool(x)\n","        # N x 1024 x 1 x 1\n","        x = torch.flatten(x, start_dim=1)\n","        # N x 1024\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","\n","        if self.aux_logits and self.training:\n","            return x, aux2, aux1\n","        return x\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","class Inception(nn.Module):\n","    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n","        super(Inception, self).__init__()\n","\n","        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n","\n","        self.branch2 = nn.Sequential(\n","            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n","            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n","        )\n","\n","        self.branch3 = nn.Sequential(\n","            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n","            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n","        )\n","\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n","            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        branch1 = self.branch1(x)\n","        branch2 = self.branch2(x)\n","        branch3 = self.branch3(x)\n","        branch4 = self.branch4(x)\n","\n","        outputs = [branch1, branch2, branch3, branch4]\n","        return torch.cat(outputs, dim=1)\n","\n","\n","class InceptionAux(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(InceptionAux, self).__init__()\n","        self.averagePool = nn.AdaptiveAvgPool2d((4, 4))\n","        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n","\n","        self.aux_classifier = nn.Sequential(\n","            nn.Linear(128 * 4 * 4, 1024),\n","            nn.Dropout(p=0.5),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.averagePool(x)\n","        x = self.conv(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = self.aux_classifier(x)\n","        return x\n","\n","\n","class BasicConv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(BasicConv2d, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        return x\n","\n"],"metadata":{"id":"V37NBoafeNkb","executionInfo":{"status":"ok","timestamp":1731182255807,"user_tz":-480,"elapsed":317,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def train_and_evaluate(model, dataset_loader, num_epochs=10, learning_rate=0.001, patience=3, device='cuda'):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    best_val_loss = np.inf\n","    patience_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(\"-\" * 20)\n","\n","        # Training Phase\n","        model.train()\n","        train_loss, train_corrects, train_preds, train_labels = 0.0, 0, [], []\n","\n","        for inputs, labels in tqdm(dataset_loader.dataloaders['train']):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            # 前向传播\n","            outputs = model(inputs)\n","            if isinstance(outputs, tuple):\n","                outputs = outputs[0]  # 只用主分支输出进行损失计算\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 更新损失和准确率\n","            train_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            train_corrects += torch.sum(preds == labels.data)\n","            train_preds.extend(preds.cpu().numpy())\n","            train_labels.extend(labels.cpu().numpy())\n","\n","        train_loss = train_loss / len(dataset_loader.image_datasets['train'])\n","        train_acc = train_corrects.double() / len(dataset_loader.image_datasets['train'])\n","        train_precision = precision_score(train_labels, train_preds, average='weighted')\n","        train_recall = recall_score(train_labels, train_preds, average='weighted')\n","        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n","        train_auc = roc_auc_score(train_labels, train_preds, multi_class='ovr')\n","\n","        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} Precision: {train_precision:.4f} Recall: {train_recall:.4f} F1: {train_f1:.4f} AUC-ROC: {train_auc:.4f}\")\n","\n","        # Validation Phase\n","        model.eval()\n","        val_loss, val_corrects, val_preds, val_labels = 0.0, 0, [], []\n","\n","        with torch.no_grad():\n","            for inputs, labels in dataset_loader.dataloaders['val']:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                outputs = model(inputs)\n","                if isinstance(outputs, tuple):\n","                    outputs = outputs[0]\n","\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_corrects += torch.sum(preds == labels.data)\n","                val_preds.extend(preds.cpu().numpy())\n","                val_labels.extend(labels.cpu().numpy())\n","\n","        val_loss = val_loss / len(dataset_loader.image_datasets['val'])\n","        val_acc = val_corrects.double() / len(dataset_loader.image_datasets['val'])\n","        val_precision = precision_score(val_labels, val_preds, average='weighted')\n","        val_recall = recall_score(val_labels, val_preds, average='weighted')\n","        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n","        val_auc = roc_auc_score(val_labels, val_preds, multi_class='ovr')\n","\n","        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} Precision: {val_precision:.4f} Recall: {val_recall:.4f} F1: {val_f1:.4f} AUC-ROC: {val_auc:.4f}\")\n","\n","        # Early Stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0  # 重置 patience 计数器\n","            best_model_wts = model.state_dict()  # 保存最佳模型权重\n","        else:\n","            patience_counter += 1\n","            print(f\"Early Stopping Counter: {patience_counter}/{patience}\")\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                model.load_state_dict(best_model_wts)  # 加载最佳模型\n","                break\n","\n","# 使用 DatasetLoader 类和训练模型\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/EE6483Project/datasets/datasets/train\"\n","val_dir = \"/content/drive/MyDrive/Colab Notebooks/EE6483Project/datasets/datasets/val\"\n","dataset_loader = DatasetLoader(train_dir, val_dir)\n","\n","# 初始化并训练模型\n","model = GoogLeNet(num_classes=2, aux_logits=True)  # 假设是二分类问题\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_and_evaluate(model, dataset_loader, num_epochs=10, learning_rate=0.001, patience=3, device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crC7HNJ_eclU","outputId":"bf03605b-c349-4603-f514-ff5b49380660"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","--------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [11:24<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6937 Acc: 0.4968 Precision: 0.4968 Recall: 0.4968 F1: 0.4968 AUC-ROC: 0.4968\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.6931 Acc: 0.5000 Precision: 0.2500 Recall: 0.5000 F1: 0.3333 AUC-ROC: 0.5000\n","Epoch 2/10\n","--------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [08:46<00:00,  2.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6933 Acc: 0.4982 Precision: 0.4982 Recall: 0.4982 F1: 0.4982 AUC-ROC: 0.4982\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Val Loss: 0.6932 Acc: 0.5000 Precision: 0.2500 Recall: 0.5000 F1: 0.3333 AUC-ROC: 0.5000\n","Early Stopping Counter: 1/3\n","Epoch 3/10\n","--------------------\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 1236/1250 [08:42<00:06,  2.33it/s]"]}]}]}